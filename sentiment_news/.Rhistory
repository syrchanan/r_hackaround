untar("20220520.CNN.1gram.txt.gz")
readLines("20220520.CNN.1gram.txt.gz")
library(tidyverse)
read_lines("20220520.CNN.1gram.txt.gz")
R.utils::gzip("20220520.CNN.1gram.txt.gz")
R.utils::gunzip("20220520.CNN.1gram.txt.gz.gz")
R.utils::gunzip("20220520.CNN.1gram.txt.gz")
R.utils::gunzip("20220520.CNN.2gram.txt.gz")
ngrams <- read_delim("20220520.CNN.1gram.txt")
ngrams <- read_delim("20220520.CNN.1gram.txt", delim = " ")
ngrams <- read_delim("20220520.CNN.1gram.txt", delim = " ", col_names = c("date", "network", "time", "word", "count"))
View(ngrams)
ngrams <- read_lines("20220520.CNN.1gram.txt")
ngrams <- read_tsv("20220520.CNN.1gram.txt")
View(ngrams)
ngrams <- read_tsv("20220520.CNN.1gram.txt", col_names = c("date", "network", "time", "word", "count"))
ngrams2 <- read_tsv("20220520.CNN.2gram.txt", col_names = c("date", "network", "time", "word", "count"))
View(ngrams2)
rm(ngrams2)
library(quanteda)
library(quanteda.textplots)
corp <- corpus(ngrams$word)
dfm <- dfm(corp, remove_punct=TRUE, remove=stopwords("english"))
textplot_wordcloud(dfm, min_count = 1)
tokens()
tokens(ngrams$word)
tokens(ngrams$word, remove_punct = T, remove_symbols = T, remove_numbers = T, remove_url = T, remove_separators = T)
tokens <- tokens(ngrams$word, remove_punct = T, remove_symbols = T, remove_numbers = T, remove_url = T, remove_separators = T)
rm(list= ls())
